# Help & FAQ

## What is A/B testing?
A/B testing is an experiment where two or more variants of a page are shown to users at random, and statistical analysis is used to determine which variation performs better for a given conversion goal.

## How does sample size affect the test?
The sample size affects the reliability of your results. A larger sample size provides more accurate results and ensures that the test has enough data to detect a meaningful difference.

## What is statistical significance?
Statistical significance is a measure of how confident you are that the results of the test are not due to random chance. It is often represented by the p-value or the alpha level.

## What is statistical power?
Statistical power is the probability that a test will detect a difference if one truly exists. It is related to the sample size, significance level, and the effect size.

## What is the Minimum Detectable Effect (MDE)?
The Minimum Detectable Effect (MDE) is the smallest change or difference that the test is designed to detect, relative to the baseline.

## What is the Baseline Conversion Rate?
The Baseline Conversion Rate is the current performance metric (such as conversion rate) that you are aiming to improve upon in an A/B test.

## What is a one-sided vs. two-sided hypothesis test?
A one-sided test is used when you are only interested in detecting an effect in one direction (e.g., increase in conversion). A two-sided test is used when you are interested in detecting an effect in both directions (either an increase or decrease).

## Why is statistical power important?
Statistical power is important because it determines how likely you are to detect a real effect if it exists. A test with low power may fail to detect important changes, while a test with high power increases the chance of finding meaningful results.
